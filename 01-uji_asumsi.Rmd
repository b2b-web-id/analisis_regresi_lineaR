# Uji Asumsi Klasik

Uji Asumsi Klasik merupakan uji prasyarat yang dilakukan sebelum melakukan analisis lebih lanjut terhadap data yang telah dikumpulkan. Pengujian asumsi klasik ini ditujukan agar dapat menghasilkan model regresi yang memenuhi kriteria BLUE. Uji asumsi klasik terdiri dari 4 uji.

## Uji Normalitas

Uji Normalitas dilakukan untuk mengetahui apakah residual terdistribusi normal atau tidak. Residual harus mengikuti distribusi normal untuk memenuhi asumsi dan kriteria BLUE. Penyebab residual tidak normal kemungkinan karena terdapat nilai ekstrim dalam data.

Terdapat beberapa metode pengujian yang biasa dirujuk, yaitu : Shapiro-Wilk[^shapiro1965], Liliefors[^lilliefors1967], dan Kolmogorov-Smirnov[^kolmogoroff1941].

### Uji Saphiro-Wilk {#ShapiroWilk}

Pada rilis saat ini hanya akan dijelaskan mengenai uji Saphiro-Wilk. Rumus:  

\begin{equation}
T_3 = \frac{1}{D} [ \sum_{i=1}^k a_i (X_n+i+1 - X_i) ]^2
\end{equation}

Keterangan:

- $D$ = Berdasarkan rumus di bawah
- $a$ = Coeffisient test Shapiro Wilk
- $X_n-i+1$ = Angka ke $n-i+1$ pada data
- $X_i$ = Angka ke i pada data

\begin{equation}
D=\sum_{i=1}^n{(X_i-\overline{X})^2}
\end{equation}

Keterangan:

- $X_i$ = angka ke i pada data
- $\overline{X}$ = rata-rata X

\begin{equation}
G=b_n+c_n+ln({T_3-d_n}/{1-T_3})
\end{equation}

Keterangan:

- $G$ = identik dengan nilai Z distribusi normal
- $T_3$ = sesuai rumus di atas
- $b_n$, $c_n$, $d_n$ = konversi statistik Shapiro-Wilk pendekatan distribusi normal

Syarat Uji:

- data berskala interval atau ratio (kuantitatif)
- data tunggal/belum dikelompokkan pada tabel distribusi
- data dari sampel random

## Uji Homoskedastisitas

Uji Homoskedastisitas dilakukan untuk mengetahui apakah ragam residual tidak konstan atau heteroskedastisitas, karena ketika terjadi heteroskedastisitas OLS estimator tak bias dan konsisten, hal ini dikarenakan tidak ada variabel prediktor yang berkorelasi dengan residual. Heteroskedastisitas menyebabkan meningkatnya varians residual yang berpengaruh terhadap hasil analisis karena nilai uji F dan uji T akan membesar menyebabkan nilai signifikan kecil.

Homoskedastisitas merupakan asumsi penting yang harus dipenuhi yang berarti bahwa varian dari error bersifat konstan. Terdapat beberapa pengujian yang biasa dirujuk, yaitu: Uji Park, Uji Glejser, Uji Korelasi Rank Spearman, dan Breusch-Pagan Test[^breusch1979].

Pada rilis saat ini hanya akan dijelaskan mengenai uji Breusch-Pagan.

### Uji Breusch-Pagan {#BreuschPagan}

Uji Breusch-Pagan digunakan untuk melihat apakah data dalam model regresi memiliki pola variabilitas yang berbeda-beda. Pengujian dilakukan untuk melihat perbedaan antara nilai prediksi dari model regresi dan nilai sebenarnya dari data lebih besar atau lebih kecil pada beberapa bagian data.

Misal pada model $y = \beta_0 + \beta_1 x + \upsilon$ didapatkan satu set nilai $\hat{\upsilon}$ sebagai residu/selisih/kesalahan nilai prediksi dimana nilai rata-rata adalah 0 (nol) dengan variansi $Var(\upsilon) = \sigma_t^2 = h(z'_t \alpha)$. Asumsi yang digunakan untuk menguji pola variabilitas kesalahan prediksi adalah nilai $\hat{\upsilon}$ tidak bergantung pada variabel independen $x$ dengan melakukan menggunakan model regresi tambahan dengan bentuk $\hat{\upsilon}^2 = \gamma_0 + \gamma_1 x + \epsilon$.

Statistik uji yang digunakan adalah

\begin{equation}
F = \frac{R^2_{{\upsilon^2}/k}}{1 - R^2_{{\upsilon^2}/{n-k-1}}} \sim F_{a,(k, n-k-1)}
\end{equation}

dimana $k$ merupakan jumlah variabel independen.

Hipotesis yang berlaku dalam pengujian adalah dimana nilai statistik uji $F$ > $F_{a,(k, n-k-1)}$ atau $p-value$ > $\alpha$ dengan kesimpulan:

- $H_0$ diterima bila variansi residu bersifat homoskedastis, dan
- $H_1$ bila variansi residu bersifat heterokesdastis.


## Uji autokorelasi

Uji autokorelasi dilakukan untuk mendeteksi adanya korelasi atau hubungan antara residual pengamatan yang satu dengan lainnya. Autokorelasi biasa terjadi pada data yang berurutan atau time series. Pada regresi apabila terjadi autokorelasi, maka dapat menggangu suatu model. Autokorelasi mengakibatkan penaksiran OLS tidak lagi memiliki varians yang minimum, meskipun koefisien taksiran regresi tetap tak bias. Salah satu uji yang dilakukan adalah Uji Durbin-Watson[^durbinwatson1971].

### Uji Durbin-Watson {#DurbinWatson}

Rumus dalam pengujian adalah

\begin{equation}
d = \frac{\sum^n_{t=2} (e_t - e_{t-1})^2}{\sum^n_{t=1} e^2_t}
\end{equation}

Hipotesis yang berlaku dalam pengujian adalah dimana nilai statistik uji $d_L > d$ dan $d_U < (4 - d)$ dimana $d_L, d_U$ diambil dari batas bawah dan batas atas pada tabel dan $d$ adalah hasil hitung dengan kriteria sebagai berikut[^sujarweni2016] :

- jika $0 < d < d_L$, berarti ada autokorelasi positif;
- jika $4 – d_L < d < 4$, berarti ada autokorelasi negatif;
- jika $2 < d < 4 - d_U$ atau $d_U < d < 2$, berarti tidak ada autokorelasi positif atau negatif; dan
- jika $dL <= d <= dU$ atau $4 - d_U ≤ d ≤ 4 - d_L$, pengujian tidak menyakinkan. Untuk itu dapat digunakan uji lain atau menambah data.

## Uji Multikolinearitas

Uji Multikolinearitas dilakukan untuk mendeteksi apakah terdapat korelasi yang sangat tinggi antara variabel independen. Ada beberapa tanda suatu regresi linear berganda memiliki masalah dengan multikolinearitas, yaitu nilai R Square tinggi, tetapi hanya ada sedikit variabel independen yang signifikan atau bahkan tidak signifikan. Pada uji ini yang biasa dilakukan adalah dengan mencari nilai faktor inflasi varians (VIF / _Variance Inflation Factors_).

Rumus[^islr] yang digunakan adalah:

\begin{equation}
VIF(\hat{\beta}_j) = \frac{1}{1-R^2_{X_j|X_{-j}}}
\end{equation}

dimana $R^2_{X_j|X_{-j}}$ adalah $R^2$ dari regresi variabel $X_j$.

Hipotesis yang berlaku pada pengujian adalah jika $R^2_{X_j|X_{-j}}$ mendekati 1 (satu) maka terdapat korelasi yang kuat atas variabel $X_j$. Semakin kuat korelasi maka angka VIF atas variabel $X_j$ juga semakin besar. Dengan semakin mendekati 1 maka nilai VIF akan semakin besar.

Beberapa catatang mengenai VIF :

- $VIF$ dimulai dari 1 s.d tak hingga;
- $VIF = 1$ menunjukkan tidak ada multikolinearitas antara variabel $x_i$ dengan variabel $x$ lainnya;
- $5 < VIF < 10$ menunjukkan adanya multikolinearitas yang cukup tinggi antara variabel $x_i$ dengan variabel $x$ lainnya; dan
- $VIF > 10$ menunjukkan adanya multikolinearitas yang tinggi antara variabel $x_i$ dengan variabel $x$ lainnya.

Bila terdapat masalah kolinearitas solusi sederhananya adalah dengan menghapus varibel yang bermasalah atau dengan menggabungkan variabel yang saling berkorelasi kuat menjadi 1 (satu) variabel.

[^shapiro1965]: Shapiro, S. S., and M. B. Wilk. “An Analysis of Variance Test for Normality (Complete Samples).” Biometrika 52, no. 3/4 (1965): 591–611. https://doi.org/10.2307/2333709.
[^lilliefors1967]: Lilliefors, Hubert W. “On the Kolmogorov-Smirnov Test for Normality with Mean and Variance Unknown.” Journal of the American Statistical Association 62, no. 318 (1967): 399–402. https://doi.org/10.2307/2283970.
[^kolmogoroff1941]: Frank J. Massey Jr. (1951) The Kolmogorov-Smirnov Test for Goodness of Fit, Journal of the American Statistical Association, 46:253, 68-78, DOI: 10.1080/01621459.1951.10500769
[^breusch1979]: Breusch, T. S., and A. R. Pagan. “A Simple Test for Heteroscedasticity and Random Coefficient Variation.” Econometrica 47, no. 5 (1979): 1287–94. https://doi.org/10.2307/1911963.
[^durbinwatson1971]: Durbin, J., and G. S. Watson. “Testing for Serial Correlation in Least Squares Regression. III.” Biometrika 58, no. 1 (1971): 1–19. https://doi.org/10.2307/2334313.
[^sujarweni2016]: Sujarweni, V. Wiratna. "Kupas tuntas penelitian akuntansi dengan SPSS." Yogyakarta : Pustaka Baru Press, 2016. ISBN: 602-376--041-4
[^islr]: James, Gareth; Witten, Daniela; Hastie, Trevor; Tibshirani, Robert (2017). An Introduction to Statistical Learning: with Applications in R. Springer Science+Business Media, New York. p 102. ISBN 978-1-4614-7138-7. DOI:10.1007/978-1-4614-7138-7
